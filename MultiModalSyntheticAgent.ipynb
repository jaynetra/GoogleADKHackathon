{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaynetra/GoogleADKHackathon/blob/main/MultiModalSyntheticAgent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Np0plMPXRvoq"
      },
      "source": [
        "# Multi modal synthetic data generator with ADK\n",
        "\n",
        "**Motivation for Multimodal synthetic data generator**\n",
        "\n",
        "Synthetic data has been around in one form or another for decades.But it is much more important as quality real world data is very hard to get access to for innovation. Many tools are available that address generation of single modality for synthetic data. e.g. for **Tabular Synthetic Data** that is very useful in healthcare/financial domains, tools such as SDV,dbtwin etc. can be used. Similarly there are innovative models for generation of images and audio. But access to meaningful connected multi modal synthetic data  is an industry problem, that can actually fuel innovation. The core idea for this project is to create a multi agent, multi modal super agent that can create multi modal synthetic data using various tools. This is designed as  **multi-agent system** with main agent delegating task for different modality to different subagents.\n",
        "\n",
        "The system design uses  a multi agent pattern to develop the **MultiModalSyntheticDataGeneratorAgent** component. This agent delegates the tasks to subagents that specialize in one modality. Following modalities are covered\n",
        "\n",
        "* Synthetic Tabular Data\n",
        "* Image creation\n",
        "* Synthetic medical notes\n",
        "\n",
        "The design is modular to add other modalities can be added easily.\n",
        "\n",
        "\n",
        "**Architecture Diagram**\n",
        "\n",
        "\n",
        "\n",
        "**Submission Requirement Checklist:**\n",
        "\n",
        "\n",
        "*   ✅ **Categories: Content creation and Generation**\n",
        "\n",
        "Designing specialized sub-agents and enabling automatic routing (`auto flow`) of user requests to the most appropriate agent within a team.\n",
        "\n",
        "*   ✅ **Multi Agent:**\n",
        "\n",
        "Crafting Python functions (`tools`) that grant agents specific abilities (like fetching data) and instructing agents on how to use them effectively.\n",
        "\n",
        "*   ✅ **Architecture Diagram:**\n",
        "\n",
        "Crafting Python functions (`tools`) that grant agents specific abilities (like fetching data) and instructing agents on how to use them effectively.\n",
        "\n",
        "*   ✅ **Tools and Frameworks: ADK:**\n",
        "\n",
        "Utilizing `Session State` and `ToolContext` to enable agents to remember information across conversational turns, leading to more contextual interactions.\n",
        "\n",
        "*   ✅ **New Project:**\n",
        "Implementing `before_model_callback` and `before_tool_callback` to inspect, modify, or block requests/tool usage based on predefined rules, enhancing application safety and control.\n",
        "\n",
        "*   ✅ **Project Hosting:**\n",
        "On collab environment. The link will be shared for judging\n",
        "\n",
        "*   ✅ **Project Text Description:**\n",
        "On collab environment. The link will be shared for judging\n",
        "\n",
        "*   ✅ **URL to public repo:**\n",
        "On collab environment. The link will be shared for judging\n",
        "\n",
        "*   ✅ **Submission Video Link public:**\n",
        "On collab environment. The link will be shared for judging\n",
        "\n",
        "\n",
        "**Future work:**\n",
        "\n",
        "*   ✅ **Connected synthetic data:** Many industries need connected synthetic data based on the industry domain that is multi modal. Understand the domain and create framework to connect the modalities\n",
        "\n",
        "*   ✅ **Scaling and cost:** Get understanding of scale and cost. While cost per token is a rule of thumb, figure out what other factors need to be understood.\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARCoeUZCRNGi",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Step 0: Setup and Installation\n",
        "# Install ADK and LiteLLM for multi-model support\n",
        "\n",
        "!pip install google-adk -q\n",
        "!pip install litellm -q\n",
        "!pip install sdv -q\n",
        "\n",
        "print(\"Installation complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbwxKypOSBkN"
      },
      "outputs": [],
      "source": [
        "# @title Step 1: Import necessary libraries\n",
        "import os\n",
        "import asyncio\n",
        "from google.adk.agents import Agent\n",
        "from google.adk.models.lite_llm import LiteLlm # For multi-model support\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.runners import Runner\n",
        "from google.genai import types # For creating message Content/Parts\n",
        "\n",
        "import warnings\n",
        "# Ignore all warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(level=logging.ERROR)\n",
        "\n",
        "print(\"Libraries imported.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mNsVI5eSDOi",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Step 2: Configure API Keys\n",
        "# --- IMPORTANT: Replace placeholders with your real API keys ---\n",
        "\n",
        "\n",
        "from google.colab import userdata\n",
        "#GOOGLE_API_KEY = userdata.get('GOOGLE_KEY')\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_KEY')\n",
        "MODEL_GEMINI_2_0_FLASH = \"gemini-2.0-flash\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3 : Create functions for different modalities"
      ],
      "metadata": {
        "id": "Ul5BI0pLfqRX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7LZM3ysSOMu"
      },
      "source": [
        "---\n",
        "\n",
        "## Modality 1: Synthetic tabular data\n",
        "\n",
        "Use sdv framework which is a open source tool to create. For hackathon purposes using the dataset in the framework that is public. In real scenarios, this step takes a lot of effort\n",
        "\n",
        "1. **A Tool:** A Python function that equips the agent with the *ability* to get synthetic data. Because there is limited resources and the goal is to understand the ADK concepts, return only 5 rows and returns as a json record structure. Since the model we use only outputs text, json is a good way to represent the output.\n",
        "\n",
        "2. **An Agent:** The AI \"brain\" that understands the user's request, knows it has a tabular data generation tool, and decides when and how to use it.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILy7YTCbSRAT",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Define the get_tabular_synthetic_data function\n",
        "# using sdv.dev so not need to deal with keys etc.\n",
        "\n",
        "import pandas as pd\n",
        "    # Mock tabular data\n",
        "from sdv.datasets.demo import download_demo\n",
        "from sdv.single_table import GaussianCopulaSynthesizer\n",
        "\n",
        "def get_tabular_synthetic_data() -> dict:\n",
        "    \"\"\"Retrieves the current weather report for a specified city.\n",
        "\n",
        "       Returns:\n",
        "        dict: A dictionary containing default dataframe to dict function.\n",
        "              Includes a 'status' key ('success' or 'error').\n",
        "     \"\"\"\n",
        "\n",
        "    real_data, metadata = download_demo(\n",
        "    modality='single_table',\n",
        "    dataset_name='fake_hotel_guests')\n",
        "\n",
        "    synthesizer = GaussianCopulaSynthesizer(metadata)\n",
        "    synthesizer.fit(data=real_data)\n",
        "\n",
        "    df_synth = synthesizer.sample(num_rows=5)\n",
        "\n",
        "    return df_synth.to_json(orient='records')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Ninz7-Yexacu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAiYeYquxaxi"
      },
      "source": [
        "---\n",
        "\n",
        "## Modality 2: Synthetic Image\n",
        "\n",
        "Use Gen AI tool kit with google's image generating model\n",
        "\n",
        "1. **A Tool:** A Python function that equips the agent with the *ability* to get synthetic image. Use google image generation model *gemini-2.0-flash-preview-image-generation*\n",
        "2. **An Agent:** The AI \"brain\" that understands the user's request, knows it has a image generation tool, and decides when and how to use it.\n",
        "\n",
        "3. Returns image as json since we are using a text gen model GEMINI FLASH model that can take multi modal but can only generate text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OV9_VgFwIlE"
      },
      "outputs": [],
      "source": [
        "# @title Define Tools for image synthetic agent\n",
        "from typing import Optional # Make sure to import Optional\n",
        "from google.adk.tools.tool_context import ToolContext\n",
        "import io\n",
        "\n",
        "\n",
        "def get_synthetic_image():\n",
        "    \"\"\"Provides a synthetic image. If a name is provided, it will be used.\n",
        "\n",
        "    Args:\n",
        "        nametool_context (ToolContext): Tool context to save as artifact.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the image.\n",
        "\n",
        "    \"\"\"\n",
        "    from google import genai\n",
        "    from google.genai import types\n",
        "    from PIL import Image\n",
        "    from io import BytesIO\n",
        "    import base64\n",
        "    import json, numpy as np\n",
        "\n",
        "\n",
        "    from PIL import Image, ImageDraw\n",
        "    width = 400\n",
        "    height = 400\n",
        "    image = None\n",
        "    #Image.new('RGB', (25,25), 'red')\n",
        "    contents = (\"generate a chest xray\")\n",
        "    client = genai.Client()\n",
        "\n",
        "    response = client.models.generate_content(model=\"gemini-2.0-flash-preview-image-generation\",contents=contents,\n",
        "                                              config=types.GenerateContentConfig(response_modalities=['TEXT', 'IMAGE']))\n",
        "\n",
        "    for part in response.candidates[0].content.parts:\n",
        "      if part.text is not None:\n",
        "        print(part.text)\n",
        "      if part.inline_data is not None:\n",
        "        image = Image.open(BytesIO((part.inline_data.data)))\n",
        "        image.save('gemini-native-image.png')\n",
        "        display(image)\n",
        "\n",
        "\n",
        "    json_data = json.dumps(np.array(image).tolist())\n",
        "\n",
        "    return json_data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAM0BqGWSTo5"
      },
      "source": [
        "---\n",
        "\n",
        "# Step 4 Create agents for different modalities\n",
        "Define the Tabular Syntetic Agent and Synthetic Image Agent\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ho1COmKSUeV"
      },
      "outputs": [],
      "source": [
        "# @title Define the Tabular Synthetic Agent\n",
        "# Use one of the model constants defined earlier\n",
        "AGENT_MODEL = MODEL_GEMINI_2_0_FLASH # Starting with Gemini\n",
        "\n",
        "synth_agent = Agent(\n",
        "    name=\"synthetic_agent_v1\",\n",
        "    model=AGENT_MODEL, # Can be a string for Gemini or a LiteLlm object\n",
        "    description=\"Provides synthetic multi modal data.\",\n",
        "    instruction=\"You are a helpful synthetic data generator, that can generate multi modal, text and images that are connected. \",\n",
        "    tools=[get_tabular_synthetic_data])# Pass the function directly\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwSqqtJswXgA"
      },
      "outputs": [],
      "source": [
        "# @title Define the Image Synthetic Agent\n",
        "# Use one of the model constants defined earlier\n",
        "AGENT_MODEL = MODEL_GEMINI_2_0_FLASH # Starting with Gemini\n",
        "\n",
        "synth_image_agent = Agent(\n",
        "    name=\"synthetic_image_agent_v1\",\n",
        "    model=AGENT_MODEL, # Can be a string for Gemini or a LiteLlm object\n",
        "    description=\"Provides synthetic multi modal data.\",\n",
        "    instruction=\"You are a helpful synthetic data generator, that can generate multi modal, text and images that are connected. If no description is given generate a xray image\",\n",
        "    tools=[get_synthetic_image])# Pass the function directly\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6 : Set up master agent and subagents, agent delegation model"
      ],
      "metadata": {
        "id": "2zXpA5kSgye-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Define the Root Agent with Sub-Agents for multimodal agent\n",
        "\n",
        "# Ensure sub-agents were created successfully before defining the root agent.\n",
        "# Also ensure the original 'get_weather' tool is defined.\n",
        "root_agent = None\n",
        "runner_root = None # Initialize runner\n",
        "multi_modal_super_agent = None\n",
        "if synth_image_agent:\n",
        "    # Let's use a capable Gemini model for the root agent to handle orchestration\n",
        "   root_agent_model = AGENT_MODEL\n",
        "   multi_modal_super_agent =  Agent(\n",
        "      name=\"multi_modal_super_agent_v1\", # Give it a new version name\n",
        "      model=root_agent_model,\n",
        "      description=\"The main multi modal  agent. Handles weather requests and delegates tabular and image generators.\",\n",
        "      instruction=\"You are the Super Muli modal synthetic Agent manager. Your primary responsibility is to provide tabular and image syntheic data. \"\n",
        "                   \"You have specialized sub-agents: \"\n",
        "                  \"1. 'synth_agent': Handles tabular data. \"\n",
        "                  \"2. 'synth_image_agent': Handles image data. \"\n",
        "                  \"Analyze the user's query. If it's a tabular synthetic request, delegate to 'synth_agent'. If it's a image, delegate to 'synth_image_agent'. \"\n",
        "                        \"For anything else, respond appropriately or state you cannot handle it.\",\n",
        "        # Key change: Link the sub-agents here!\n",
        "      sub_agents=[synth_agent,synth_image_agent]\n",
        "  )\n",
        "   print(f\"✅ Root Agent '{multi_modal_super_agent.name}' created using model '{root_agent_model}' \")\n",
        "else:\n",
        "    print(\"❌ Cannot create root agent because one or more sub-agents failed to initialize .\")\n"
      ],
      "metadata": {
        "id": "roPuxMtbymPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 7 : How is the agent interact with the function in collab environment"
      ],
      "metadata": {
        "id": "dgPuX-7Sg9Qc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZJr8lbkSebH"
      },
      "outputs": [],
      "source": [
        "# @title Define Agent Interaction Function\n",
        "\n",
        "from google.genai import types # For creating message Content/Parts\n",
        "\n",
        "async def call_agent_async(query: str, runner, user_id, session_id):\n",
        "  \"\"\"Sends a query to the agent and prints the final response.\"\"\"\n",
        "  print(f\"\\n>>> User Query: {query}\")\n",
        "\n",
        "\n",
        "  # Prepare the user's message in ADK format\n",
        "  content = types.Content(role='user', parts=[types.Part(text=query)])\n",
        "\n",
        "\n",
        "\n",
        "  final_response_text = \"Agent did not produce a final response.\" # Default\n",
        "\n",
        "  # Key Concept: run_async executes the agent logic and yields Events.\n",
        "  # We iterate through events to find the final answer.\n",
        "  async for event in runner.run_async(user_id=user_id, session_id=session_id, new_message=content):\n",
        "      # You can uncomment the line below to see *all* events during execution\n",
        "      print(f\"  [Event] Author: {event.author}, Type: {type(event).__name__}, Final: {event.is_final_response()}, Content: {event.content}\")\n",
        "      print(event.actions.state_delta)\n",
        "      print(\"**** artifact delta *****\")\n",
        "      print(event.actions.artifact_delta)\n",
        "\n",
        "      # Key Concept: is_final_response() marks the concluding message for the turn.\n",
        "      if event.is_final_response():\n",
        "          if event.content and event.content.parts:\n",
        "             # Assuming text response in the first part\n",
        "             final_response_text = event.content.parts[0].text\n",
        "          elif event.actions and event.actions.escalate: # Handle potential errors/escalations\n",
        "             final_response_text = f\"Agent escalated: {event.error_message or 'No specific message.'}\"\n",
        "          # Add more checks here if needed (e.g., specific error codes)\n",
        "          break # Stop processing events once the final response is found\n",
        "\n",
        "\n",
        "  print(f\"<<< Agent Response: {final_response_text}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 8 :Finally set up super agent, its session and some prompts to verify if the subagents are called correctly"
      ],
      "metadata": {
        "id": "Ij5RFgfWhX0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Interact with the Super Agent\n",
        "import asyncio # Ensure asyncio is imported\n",
        "from google.adk.sessions import InMemorySessionService\n",
        "from google.adk.artifacts import InMemoryArtifactService\n",
        "from google.adk.runners import Runner\n",
        "\n",
        "# Ensure the root agent (e.g., 'weather_agent_team' or 'root_agent' from the previous cell) is defined.\n",
        "# Ensure the call_agent_async function is defined.\n",
        "\n",
        "# Check if the root agent variable exists before defining the conversation function\n",
        "root_agent_var_name = 'root_agent' # Default name from Step 3 guide\n",
        "if 'multi_modal_super_agent' in globals(): # Check if user used this name instead\n",
        "    root_agent_var_name = 'multi_modal_super_agent'\n",
        "elif 'root_agent' not in globals():\n",
        "    print(\"⚠️ Root agent ('root_agent' or 'weather_agent_team') not found. Cannot define run_team_conversation.\")\n",
        "    # Assign a dummy value to prevent NameError later if the code block runs anyway\n",
        "    root_agent = None # Or set a flag to prevent execution\n",
        "\n",
        "# Only define and run if the root agent exists\n",
        "if root_agent_var_name in globals() and globals()[root_agent_var_name]:\n",
        "    # Define the main async function for the conversation logic.\n",
        "    # The 'await' keywords INSIDE this function are necessary for async operations.\n",
        "    async def run_synthetic_conversation():\n",
        "        print(\"\\n--- Testing Agent Team Delegation ---\")\n",
        "        session_service = InMemorySessionService()\n",
        "        artifact_service =InMemoryArtifactService()\n",
        "        # Choose an implementation\n",
        "\n",
        "        APP_NAME = \"multi_modal_synthetic_agent\"\n",
        "        USER_ID = \"user_1_synthetica_agent\"\n",
        "        SESSION_ID = \"session_001_synthetic_agent\"\n",
        "        session = await session_service.create_session(\n",
        "            app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID\n",
        "        )\n",
        "        print(f\"Session created: App='{APP_NAME}', User='{USER_ID}', Session='{SESSION_ID}'\")\n",
        "\n",
        "        actual_root_agent = globals()[root_agent_var_name]\n",
        "        runner_agent_team = Runner( # Or use InMemoryRunner\n",
        "            agent=actual_root_agent,\n",
        "            app_name=APP_NAME,\n",
        "            session_service=session_service,\n",
        "            artifact_service=artifact_service\n",
        "         )\n",
        "        print(f\"Runner created for agent '{actual_root_agent.name}'.\")\n",
        "\n",
        "        # --- Interactions using await (correct within async def) ---\n",
        "\n",
        "\n",
        "        await call_agent_async(query = \"generate an image\",\n",
        "                               runner=runner_agent_team,\n",
        "                               user_id=USER_ID,\n",
        "                               session_id=SESSION_ID)\n",
        "\n",
        "        await call_agent_async(query = \"generate tabular data\",\n",
        "                               runner=runner_agent_team,\n",
        "                               user_id=USER_ID,\n",
        "                               session_id=SESSION_ID)\n",
        "\n",
        "\n",
        "print(\"Attempting execution using 'await' (default for notebooks)...\")\n",
        "await run_synthetic_conversation()\n",
        "\n"
      ],
      "metadata": {
        "id": "L54VIqbSh6KM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Next Steps: The above 8 steps are the MVP functionalities to get an understanding of ADK. Still to experiment, deploy on Vertex AI, storing images using artifact service, applying guardrails etc."
      ],
      "metadata": {
        "id": "Q3CrsyrdhxOn"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}